# Par Function Benchmark Results

## Executive Summary

The optimized `par` function implementation provides **significant performance improvements** across all test cases:
- **Average Speed Improvement: 27.8%**
- **Average Allocation Reduction: 49.2%**

## Detailed Results

### Test Case 1: 5-Year Semi-Annual
```
Original:  median=1053.0ns, allocs=688 bytes
Optimized: median=691.0ns,  allocs=288 bytes
âš¡ Speed improvement: 34.4%
ðŸ’¾ Allocation reduction: 58.1%
```

### Test Case 2: 10-Year Semi-Annual
```
Original:  median=1413.0ns, allocs=928 bytes
Optimized: median=992.0ns,  allocs=448 bytes
âš¡ Speed improvement: 29.8%
ðŸ’¾ Allocation reduction: 51.7%
```

### Test Case 3: 30-Year Semi-Annual
```
Original:  median=3036.0ns, allocs=1984 bytes
Optimized: median=2354.0ns, allocs=1152 bytes
âš¡ Speed improvement: 22.5%
ðŸ’¾ Allocation reduction: 41.9%
```

### Test Case 4: 10-Year Quarterly
```
Original:  median=2274.0ns, allocs=1456 bytes
Optimized: median=1713.0ns, allocs=800 bytes
âš¡ Speed improvement: 24.7%
ðŸ’¾ Allocation reduction: 45.1%
```

## Summary Table

| Test Case                | Speed Improvement | Allocation Reduction |
|--------------------------|-------------------|----------------------|
| 5-year semi-annual       | 34.4%            | 58.1%               |
| 10-year semi-annual      | 29.8%            | 51.7%               |
| 30-year semi-annual      | 22.5%            | 41.9%               |
| 10-year quarterly        | 24.7%            | 45.1%               |
| **Average**              | **27.8%**        | **49.2%**           |

## Key Insights

1. **Shorter maturities benefit more**: The 5-year case shows the largest speed improvement (34.4%), as the relative overhead of array operations is higher with fewer cash flows.

2. **Consistent memory optimization**: All cases show substantial allocation reduction (41.9% - 58.1%), demonstrating effective memory optimization through pre-allocated arrays.

3. **Scalable improvements**: Performance gains are consistent across different maturities and frequencies.

## Technical Details

### Original Implementation
```julia
cfs = [t == last(coup_times) ? 1 + r : r for t in coup_times]  # Array comprehension
cfs = [-1; cfs]  # Array concatenation
times = [0; coup_times]  # Array concatenation with collect
```
- Creates 3-4 temporary arrays
- Multiple allocation/reallocation cycles
- Overhead from array concatenation

### Optimized Implementation
```julia
n = length(coup_times)
cfs = Vector{typeof(r)}(undef, n + 1)
times = Vector{typeof(Î”t)}(undef, n + 1)
@inbounds for i in 1:n
    cfs[i + 1] = i == n ? 1 + r : r
    times[i + 1] = coup_times[i]
end
```
- Pre-allocated arrays (exact size)
- Single loop to populate both arrays
- `@inbounds` eliminates bounds checking overhead
- Better cache locality

## Conclusion

The benchmark clearly demonstrates that the optimized implementation is superior across all scenarios, providing:
- **~28% faster execution** on average
- **~49% fewer memory allocations** on average
- More predictable performance characteristics
- Better scalability for longer maturities
